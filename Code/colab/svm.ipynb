{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhuRRPLa5VDpySJfGNFnmP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jj-source/FakeNews/blob/main/codice/colab/svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHjOVBkYrAWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71658f3b-8def-4664-e77a-cf45deff0064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbKrwqaXrZgf",
        "outputId": "168321fc-ed0f-4206-b69f-ea21acecd986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.22.4)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "#Import stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#Import svm model\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# ranking elements\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from rank_bm25 import BM25Plus\n",
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "e1iDBrl0ulfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "#extracting stop-words\n",
        "nltk.download('stopwords')\n",
        "it_stopwords = stopwords.words(\"italian\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAb2HKWHwi2M",
        "outputId": "ca12b617-c6b3-462c-ee8e-18376134a9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/gdrive/MyDrive/Colab Notebooks/fakenews_data/splits/verdetto/join_d1/\""
      ],
      "metadata": {
        "id": "w2c84H8guqw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estrazione split da Drive**"
      ],
      "metadata": {
        "id": "9gClVt-vCIAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\"training_data\", \"training_labels\", \"dev_data\", \"dev_labels\"]\n",
        "\n",
        "apiContent = open(path + \"training_data.json\")\n",
        "training_data = json.load(apiContent)\n",
        "apiContent.close()\n",
        "\n",
        "apiContent = open(path + \"training_labels.json\")\n",
        "training_labels = json.load(apiContent)\n",
        "apiContent.close()\n",
        "\n",
        "apiContent = open(path + \"dev_data.json\")\n",
        "dev_data = json.load(apiContent)\n",
        "apiContent.close()\n",
        "\n",
        "apiContent = open(path + \"dev_labels.json\")\n",
        "dev_labels = json.load(apiContent)\n",
        "apiContent.close()\n",
        "\n",
        "# total data\n",
        "apiContent = open(\"/content/gdrive/MyDrive/Colab Notebooks/fakenews_data/github/join_d1.json\")\n",
        "all_data = json.load(apiContent)\n",
        "apiContent.close()"
      ],
      "metadata": {
        "id": "njLfBmczxv6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Veririca bias nel dataset**\n",
        "\n",
        "data : statement\n",
        "\n",
        "label : verdict"
      ],
      "metadata": {
        "id": "GImyji2rGzWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ridurre il data dello split ai soli statement"
      ],
      "metadata": {
        "id": "ZWy9DCI6HGOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_statements = []\n",
        "dev_statements = []\n",
        "\n",
        "for element in training_data:\n",
        "  training_statements.append(element[\"statement\"])\n",
        "for element in dev_data:\n",
        "  dev_statements.append(element[\"statement\"])"
      ],
      "metadata": {
        "id": "fhLkdq2b92-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mapping delle labels su interi**\n",
        "\n",
        "forse necessario, forse no. lo salvo qui per ora"
      ],
      "metadata": {
        "id": "NZ3UHQ7qIKHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure that the labels are encoded as numerical values, such as integers or one-hot encoding\n",
        "def numerazione(Y):\n",
        "  Y1 = []\n",
        "  for esito in Y:\n",
        "    if esito == \"Vero\":\n",
        "      Y1.append(0)\n",
        "    elif esito == \"Falso\":\n",
        "      Y1.append(2)\n",
        "    elif esito == \"Nì\":\n",
        "      Y1.append(1)\n",
        "  return Y1"
      ],
      "metadata": {
        "id": "GnaERBmPIJaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**vettorizziamo l'input usando tfidf**"
      ],
      "metadata": {
        "id": "htupiUrbiXnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words = it_stopwords)\n",
        "tfidf.fit(training_statements)"
      ],
      "metadata": {
        "id": "nTDJXWrriQvj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "a40fe57e-f52f-46f2-b719-268ec7dae18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(ngram_range=(1, 2),\n",
              "                stop_words=['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl',\n",
              "                            'alla', 'alle', 'con', 'col', 'coi', 'da', 'dal',\n",
              "                            'dallo', 'dai', 'dagli', 'dall', 'dagl', 'dalla',\n",
              "                            'dalle', 'di', 'del', 'dello', 'dei', 'degli',\n",
              "                            'dell', 'degl', 'della', 'delle', ...])"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
              "                stop_words=[&#x27;ad&#x27;, &#x27;al&#x27;, &#x27;allo&#x27;, &#x27;ai&#x27;, &#x27;agli&#x27;, &#x27;all&#x27;, &#x27;agl&#x27;,\n",
              "                            &#x27;alla&#x27;, &#x27;alle&#x27;, &#x27;con&#x27;, &#x27;col&#x27;, &#x27;coi&#x27;, &#x27;da&#x27;, &#x27;dal&#x27;,\n",
              "                            &#x27;dallo&#x27;, &#x27;dai&#x27;, &#x27;dagli&#x27;, &#x27;dall&#x27;, &#x27;dagl&#x27;, &#x27;dalla&#x27;,\n",
              "                            &#x27;dalle&#x27;, &#x27;di&#x27;, &#x27;del&#x27;, &#x27;dello&#x27;, &#x27;dei&#x27;, &#x27;degli&#x27;,\n",
              "                            &#x27;dell&#x27;, &#x27;degl&#x27;, &#x27;della&#x27;, &#x27;delle&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
              "                stop_words=[&#x27;ad&#x27;, &#x27;al&#x27;, &#x27;allo&#x27;, &#x27;ai&#x27;, &#x27;agli&#x27;, &#x27;all&#x27;, &#x27;agl&#x27;,\n",
              "                            &#x27;alla&#x27;, &#x27;alle&#x27;, &#x27;con&#x27;, &#x27;col&#x27;, &#x27;coi&#x27;, &#x27;da&#x27;, &#x27;dal&#x27;,\n",
              "                            &#x27;dallo&#x27;, &#x27;dai&#x27;, &#x27;dagli&#x27;, &#x27;dall&#x27;, &#x27;dagl&#x27;, &#x27;dalla&#x27;,\n",
              "                            &#x27;dalle&#x27;, &#x27;di&#x27;, &#x27;del&#x27;, &#x27;dello&#x27;, &#x27;dei&#x27;, &#x27;degli&#x27;,\n",
              "                            &#x27;dell&#x27;, &#x27;degl&#x27;, &#x27;della&#x27;, &#x27;delle&#x27;, ...])</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_statements = tfidf.transform(training_statements)\n",
        "print(training_statements.shape)\n",
        "dev_statements = tfidf.transform(dev_statements)\n",
        "print(dev_statements.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVNyCgamJAgh",
        "outputId": "6af659a0-000f-4094-c5da-61cd3c00104f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3219, 48105)\n",
            "(358, 48105)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from imblearn.over_sampling import RandomOverSampler\n",
        "#X_train, y_train = RandomOverSampler(random_state=123).fit_resample(X_train, y_train)\n",
        "#potrebbe risultare utile per bilanciare i falsi che sono pochi"
      ],
      "metadata": {
        "id": "bv8JTYnii1ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a svm Classifier\n",
        "clf = svm.SVC(random_state=42, kernel='linear') # Linear Kernel"
      ],
      "metadata": {
        "id": "y3GUeb6wFrUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(training_statements,training_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "I9ViSCt0jUm1",
        "outputId": "fb9f5c1d-1a88-40c4-dff9-016c92ca97a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear', random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clf.predict\n",
        "# scikitlear.metrics e classification report\n",
        "# gli passi le etichette vere, quelle ottenute.\n",
        "# ! modifica parametro digit = 4 per avere più numeri decimali"
      ],
      "metadata": {
        "id": "lcTOEhiKjdSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_labels = clf.predict(dev_statements)"
      ],
      "metadata": {
        "id": "JNyKUPIsPm5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['Vero', 'Nì', 'Falso']\n",
        "print(classification_report(dev_labels, prediction_labels, target_names=target_names, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZYyqP-LQCj6",
        "outputId": "601fe699-cbee-42bf-9931-74c436ac4f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Vero     0.3333    0.0864    0.1373        81\n",
            "          Nì     0.4788    0.7483    0.5840       151\n",
            "       Falso     0.4356    0.3492    0.3877       126\n",
            "\n",
            "    accuracy                         0.4581       358\n",
            "   macro avg     0.4159    0.3947    0.3696       358\n",
            "weighted avg     0.4307    0.4581    0.4138       358\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FASE 2.5: AGGIUNGENDO IL POLITICO CAMBIA QUALCOSA?**"
      ],
      "metadata": {
        "id": "Yna8lzkjv9Os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit the one hot encoder on all the politicians on the dataset, indipendently of the split.\n",
        "then transform the split"
      ],
      "metadata": {
        "id": "SZU-lZ_HZ3PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_politicians = []\n",
        "dev_politicians = []\n",
        "for element in training_data:\n",
        "  training_politicians.append(element[\"politician\"])\n",
        "for element in dev_data:\n",
        "  dev_politicians.append(element[\"politician\"])"
      ],
      "metadata": {
        "id": "UNM62Bu5V3_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(\"/content/gdrive/MyDrive/Colab Notebooks/onehot.json\", 'w') as json_file:\n",
        "  #json.dump(q1, json_file, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "xLkVW3_VjgZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training_politicians_bin = lbn.transform(training_politicians)\n",
        "#dev_politicians_bin = lbn.transform(dev_politicians)"
      ],
      "metadata": {
        "id": "ZwWb8egZXCs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding vectorization dei politici\n",
        "# poi incolli i due vettori\n",
        "# !! togli stop words importate da nltk"
      ],
      "metadata": {
        "id": "l2FRxPKh1qPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statement_transformer = Pipeline(\n",
        "    steps=[('encoder', TfidfVectorizer(ngram_range=(1, 2), stop_words = it_stopwords))]\n",
        ")\n",
        "\n",
        "pol_transformer = Pipeline(\n",
        "    steps=[('encoder', OneHotEncoder(handle_unknown = \"ignore\"))]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"stm\", statement_transformer,\"statements\"),\n",
        "        (\"pol\", pol_transformer, [\"politicians\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', svm.SVC(random_state=42, kernel='linear'))\n",
        "])"
      ],
      "metadata": {
        "id": "fHTjs3dSpZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# devo unire statements e politicians\n",
        "df = pd.DataFrame()\n",
        "df[\"statements\"]  = training_statements\n",
        "df[\"politicians\"]  = training_politicians#[i.split(' ') for i in training_politicians]"
      ],
      "metadata": {
        "id": "DBu6RHStqj0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"******************** Training data ********************\")\n",
        "#display(df)\n",
        "#preprocessor.fit(df)\n",
        "#display((preprocessor.transform(df)))"
      ],
      "metadata": {
        "id": "VdxoO_SMw6RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(df, training_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "O0vLk9sJG1EY",
        "outputId": "cb97fdee-d4c3-4603-d55d-051ab0df851a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('stm',\n",
              "                                                  Pipeline(steps=[('encoder',\n",
              "                                                                   TfidfVectorizer(ngram_range=(1,\n",
              "                                                                                                2),\n",
              "                                                                                   stop_words=['ad',\n",
              "                                                                                               'al',\n",
              "                                                                                               'allo',\n",
              "                                                                                               'ai',\n",
              "                                                                                               'agli',\n",
              "                                                                                               'all',\n",
              "                                                                                               'agl',\n",
              "                                                                                               'alla',\n",
              "                                                                                               'alle',\n",
              "                                                                                               'con',\n",
              "                                                                                               'col',\n",
              "                                                                                               'coi',\n",
              "                                                                                               'da',\n",
              "                                                                                               'dal',\n",
              "                                                                                               'dallo',\n",
              "                                                                                               'dai',\n",
              "                                                                                               'dagli',\n",
              "                                                                                               'dall',\n",
              "                                                                                               'dagl',\n",
              "                                                                                               'dalla',\n",
              "                                                                                               'dalle',\n",
              "                                                                                               'di',\n",
              "                                                                                               'del',\n",
              "                                                                                               'dello',\n",
              "                                                                                               'dei',\n",
              "                                                                                               'degli',\n",
              "                                                                                               'dell',\n",
              "                                                                                               'degl',\n",
              "                                                                                               'della',\n",
              "                                                                                               'delle', ...]))]),\n",
              "                                                  'statements'),\n",
              "                                                 ('pol',\n",
              "                                                  Pipeline(steps=[('encoder',\n",
              "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
              "                                                  ['politicians'])])),\n",
              "                ('model', SVC(kernel='linear', random_state=42))])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;stm&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
              "                                                                   TfidfVectorizer(ngram_range=(1,\n",
              "                                                                                                2),\n",
              "                                                                                   stop_words=[&#x27;ad&#x27;,\n",
              "                                                                                               &#x27;al&#x27;,\n",
              "                                                                                               &#x27;allo&#x27;,\n",
              "                                                                                               &#x27;ai&#x27;,\n",
              "                                                                                               &#x27;agli&#x27;,\n",
              "                                                                                               &#x27;all&#x27;,\n",
              "                                                                                               &#x27;agl&#x27;,\n",
              "                                                                                               &#x27;alla&#x27;,\n",
              "                                                                                               &#x27;alle&#x27;,\n",
              "                                                                                               &#x27;con&#x27;,\n",
              "                                                                                               &#x27;col&#x27;,\n",
              "                                                                                               &#x27;coi&#x27;,\n",
              "                                                                                               &#x27;da&#x27;,\n",
              "                                                                                               &#x27;dal&#x27;,\n",
              "                                                                                               &#x27;dallo&#x27;,\n",
              "                                                                                               &#x27;dai&#x27;,\n",
              "                                                                                               &#x27;dagli&#x27;,\n",
              "                                                                                               &#x27;dall&#x27;,\n",
              "                                                                                               &#x27;dagl&#x27;,\n",
              "                                                                                               &#x27;dalla&#x27;,\n",
              "                                                                                               &#x27;dalle&#x27;,\n",
              "                                                                                               &#x27;di&#x27;,\n",
              "                                                                                               &#x27;del&#x27;,\n",
              "                                                                                               &#x27;dello&#x27;,\n",
              "                                                                                               &#x27;dei&#x27;,\n",
              "                                                                                               &#x27;degli&#x27;,\n",
              "                                                                                               &#x27;dell&#x27;,\n",
              "                                                                                               &#x27;degl&#x27;,\n",
              "                                                                                               &#x27;della&#x27;,\n",
              "                                                                                               &#x27;delle&#x27;, ...]))]),\n",
              "                                                  &#x27;statements&#x27;),\n",
              "                                                 (&#x27;pol&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
              "                                                  [&#x27;politicians&#x27;])])),\n",
              "                (&#x27;model&#x27;, SVC(kernel=&#x27;linear&#x27;, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;stm&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
              "                                                                   TfidfVectorizer(ngram_range=(1,\n",
              "                                                                                                2),\n",
              "                                                                                   stop_words=[&#x27;ad&#x27;,\n",
              "                                                                                               &#x27;al&#x27;,\n",
              "                                                                                               &#x27;allo&#x27;,\n",
              "                                                                                               &#x27;ai&#x27;,\n",
              "                                                                                               &#x27;agli&#x27;,\n",
              "                                                                                               &#x27;all&#x27;,\n",
              "                                                                                               &#x27;agl&#x27;,\n",
              "                                                                                               &#x27;alla&#x27;,\n",
              "                                                                                               &#x27;alle&#x27;,\n",
              "                                                                                               &#x27;con&#x27;,\n",
              "                                                                                               &#x27;col&#x27;,\n",
              "                                                                                               &#x27;coi&#x27;,\n",
              "                                                                                               &#x27;da&#x27;,\n",
              "                                                                                               &#x27;dal&#x27;,\n",
              "                                                                                               &#x27;dallo&#x27;,\n",
              "                                                                                               &#x27;dai&#x27;,\n",
              "                                                                                               &#x27;dagli&#x27;,\n",
              "                                                                                               &#x27;dall&#x27;,\n",
              "                                                                                               &#x27;dagl&#x27;,\n",
              "                                                                                               &#x27;dalla&#x27;,\n",
              "                                                                                               &#x27;dalle&#x27;,\n",
              "                                                                                               &#x27;di&#x27;,\n",
              "                                                                                               &#x27;del&#x27;,\n",
              "                                                                                               &#x27;dello&#x27;,\n",
              "                                                                                               &#x27;dei&#x27;,\n",
              "                                                                                               &#x27;degli&#x27;,\n",
              "                                                                                               &#x27;dell&#x27;,\n",
              "                                                                                               &#x27;degl&#x27;,\n",
              "                                                                                               &#x27;della&#x27;,\n",
              "                                                                                               &#x27;delle&#x27;, ...]))]),\n",
              "                                                  &#x27;statements&#x27;),\n",
              "                                                 (&#x27;pol&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
              "                                                  [&#x27;politicians&#x27;])])),\n",
              "                (&#x27;model&#x27;, SVC(kernel=&#x27;linear&#x27;, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;stm&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
              "                                                  TfidfVectorizer(ngram_range=(1,\n",
              "                                                                               2),\n",
              "                                                                  stop_words=[&#x27;ad&#x27;,\n",
              "                                                                              &#x27;al&#x27;,\n",
              "                                                                              &#x27;allo&#x27;,\n",
              "                                                                              &#x27;ai&#x27;,\n",
              "                                                                              &#x27;agli&#x27;,\n",
              "                                                                              &#x27;all&#x27;,\n",
              "                                                                              &#x27;agl&#x27;,\n",
              "                                                                              &#x27;alla&#x27;,\n",
              "                                                                              &#x27;alle&#x27;,\n",
              "                                                                              &#x27;con&#x27;,\n",
              "                                                                              &#x27;col&#x27;,\n",
              "                                                                              &#x27;coi&#x27;,\n",
              "                                                                              &#x27;da&#x27;,\n",
              "                                                                              &#x27;dal&#x27;,\n",
              "                                                                              &#x27;dallo&#x27;,\n",
              "                                                                              &#x27;dai&#x27;,\n",
              "                                                                              &#x27;dagli&#x27;,\n",
              "                                                                              &#x27;dall&#x27;,\n",
              "                                                                              &#x27;dagl&#x27;,\n",
              "                                                                              &#x27;dalla&#x27;,\n",
              "                                                                              &#x27;dalle&#x27;,\n",
              "                                                                              &#x27;di&#x27;,\n",
              "                                                                              &#x27;del&#x27;,\n",
              "                                                                              &#x27;dello&#x27;,\n",
              "                                                                              &#x27;dei&#x27;,\n",
              "                                                                              &#x27;degli&#x27;,\n",
              "                                                                              &#x27;dell&#x27;,\n",
              "                                                                              &#x27;degl&#x27;,\n",
              "                                                                              &#x27;della&#x27;,\n",
              "                                                                              &#x27;delle&#x27;, ...]))]),\n",
              "                                 &#x27;statements&#x27;),\n",
              "                                (&#x27;pol&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
              "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
              "                                 [&#x27;politicians&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">stm</label><div class=\"sk-toggleable__content\"><pre>statements</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
              "                stop_words=[&#x27;ad&#x27;, &#x27;al&#x27;, &#x27;allo&#x27;, &#x27;ai&#x27;, &#x27;agli&#x27;, &#x27;all&#x27;, &#x27;agl&#x27;,\n",
              "                            &#x27;alla&#x27;, &#x27;alle&#x27;, &#x27;con&#x27;, &#x27;col&#x27;, &#x27;coi&#x27;, &#x27;da&#x27;, &#x27;dal&#x27;,\n",
              "                            &#x27;dallo&#x27;, &#x27;dai&#x27;, &#x27;dagli&#x27;, &#x27;dall&#x27;, &#x27;dagl&#x27;, &#x27;dalla&#x27;,\n",
              "                            &#x27;dalle&#x27;, &#x27;di&#x27;, &#x27;del&#x27;, &#x27;dello&#x27;, &#x27;dei&#x27;, &#x27;degli&#x27;,\n",
              "                            &#x27;dell&#x27;, &#x27;degl&#x27;, &#x27;della&#x27;, &#x27;delle&#x27;, ...])</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pol</label><div class=\"sk-toggleable__content\"><pre>[&#x27;politicians&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev = pd.DataFrame()\n",
        "df_dev[\"statements\"]  = dev_statements\n",
        "df_dev[\"politicians\"]  = dev_politicians"
      ],
      "metadata": {
        "id": "3CkLRcvMzymz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_labels = pipe.predict(df_dev)\n",
        "target_names = ['Vero', 'Nì', 'Falso']\n",
        "print(classification_report(dev_labels, prediction_labels, target_names=target_names, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcratl8ty8SU",
        "outputId": "5e773316-093a-427f-e806-27b14e3c8cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Vero     0.4259    0.2840    0.3407        81\n",
            "          Nì     0.4946    0.6093    0.5460       151\n",
            "       Falso     0.5085    0.4762    0.4918       126\n",
            "\n",
            "    accuracy                         0.4888       358\n",
            "   macro avg     0.4763    0.4565    0.4595       358\n",
            "weighted avg     0.4840    0.4888    0.4805       358\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FASE 3:  RANKING TASK**\n",
        "\n",
        "dato il claim noi abbiamo una quantità di testi associata che spiega perchè è vero o falso. questo testo (content) è rilevante per questo claim. l'algoritmo deve imparare a riconoscere data una frase quale contenuto ampio è il più simile.\n",
        "test:\n",
        "input: lista di claim, lista di spiegazioni\n",
        "associare claim a spiegazione\n",
        "valutazione: recall impossibile da stabilire perchè la recall è il mondo esterno. recall alta = sovrageneralizzazione, la recall perfetta è 1.\n",
        "per i task di ranking si guarda la 'mean average precision a x' : ranking di tutti i testi e poi verifichi se il documento corretto è nei primi x testi (parti da 1, poi 3 e così via)\n",
        "\n",
        "imbellimento: tool di generazione che cambia il claim in modo molto simile ma non esattamente la stessa\n"
      ],
      "metadata": {
        "id": "VVHQ8EbUvvd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "modello chiamato RANKBM25\n",
        "\n",
        "pulire un pochettino i dati di input (lieve preprocessing ex: rimuovere gli url, le stopwords, [idea] tfidf e conservare come token solo quelli con tfidf sopra una certa soglia)\n",
        "\n",
        "obiettivo: dato claim trovare evidence (usa mean averadge precision a k)\n",
        "\n",
        "bm25 sarà la nostra baseline (leggi paper a riguardo per capire come funzione bm25)\n",
        "\n",
        "[bonus] fai un dummy per vedere la differenza\n",
        "\n",
        "[schiacciasassi per aprire una noce] esperimento con large language model: ma il nostro dataset ha univocità dei claim ed i modelli italiani già disponibili fan cagare"
      ],
      "metadata": {
        "id": "cceKVnYJ2EMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing: \n",
        "\n",
        "\n",
        "1. niente\n",
        "2. stemming\n",
        "3. stemming + stop words\n",
        "4. tfdidf per i content\n",
        "\n",
        "! *Stemming identifies the common root form of a word by removing or replacing word suffixes (e.g. “flooding” is stemmed as “flood”), while lemmatization identifies the inflected forms of a word and returns its base form (e.g. “better” is lemmatized as “good”).*\n"
      ],
      "metadata": {
        "id": "sEoB8nqpnSP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/57592503/italian-stemmer-alternative-to-snowball"
      ],
      "metadata": {
        "id": "V4kYMAPTBqU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_file = \"/content/gdrive/MyDrive/Colab Notebooks/fakenews_data/github/join_d1.json\"\n",
        "ds = open(dataset_file)\n",
        "data = json.load(ds)\n",
        "ds.close()"
      ],
      "metadata": {
        "id": "YaRG12ZKPFBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/gdrive/MyDrive/Colab Notebooks/fakenews_data/splits/verdetto/join_d1/\"\n",
        "\n",
        "apiContent = open(path + \"dev_data.json\")\n",
        "dev_data = json.load(apiContent)\n",
        "apiContent.close()\n",
        "\n",
        "apiContent = open(path + \"dev_labels.json\")\n",
        "dev_labels = json.load(apiContent)\n",
        "apiContent.close()"
      ],
      "metadata": {
        "id": "5Sc4LvsQfmj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)**\n",
        "\n",
        " procedimento: tokenizza i docs, li dai a bm25, tokenizza la query e poi usa bm25"
      ],
      "metadata": {
        "id": "zmITdDHZcC8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = []\n",
        "corpus = []\n",
        "\n",
        "for articolo in dev_data:\n",
        "  doc = \" \".join(articolo[\"content\"])\n",
        "  tokenized_corpus.append(wordpunct_tokenize(doc))\n",
        "  corpus.append(doc)"
      ],
      "metadata": {
        "id": "yOAddlZGcGKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = BM25Plus(tokenized_corpus)"
      ],
      "metadata": {
        "id": "_eAVyoAggVwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_list = [wordpunct_tokenize(article[\"statement\"]) for article in dev_data]"
      ],
      "metadata": {
        "id": "215FpWNkiok6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stampa dell'accuratezza paragonando il documento giusto (this_corpus[idx]) con\n",
        "# i primi n doc trovati dal bm25 tramite la funzione get_top_n\n",
        "\n",
        "def k_check_simple(n, this_corpus):\n",
        "  vero = 0\n",
        "  for idx in range(len(claim_list)):\n",
        "    res = bm25.get_top_n(claim_list[idx], this_corpus, n)\n",
        "  # lista di n doc\n",
        "    for doc in res:\n",
        "      if \" \".join(doc) == \" \".join(this_corpus[idx]):\n",
        "        vero += 1\n",
        "        break\n",
        "  return vero / len(claim_list)\n",
        "\n",
        "# identito al metodo sopra ma stampa per ogni query:\n",
        "# - la query\n",
        "# - i top n documenti trovati dal bm25\n",
        "# - il documento originale\n",
        "# - \"found\" : 1 se c'è corrispondenza, 0 se no\n",
        "# additional_path è il nome della cartella in cui salverà i risultati\n",
        "def k_check(n, this_corpus, additional_path):\n",
        "  match_list = []\n",
        "  found_indexes_list = []\n",
        "  vero = 0\n",
        "  for idx in range(len(claim_list)):\n",
        "    res = bm25.get_top_n(claim_list[idx], this_corpus, n)\n",
        "    match_list.append(res)\n",
        "  # lista di n doc\n",
        "    old_vero = vero\n",
        "    for doc in res:\n",
        "      if \" \".join(doc) == \" \".join(this_corpus[idx]):\n",
        "        vero += 1\n",
        "        break\n",
        "    found_indexes_list.append(vero-old_vero)\n",
        "  confronto(n, match_list, this_corpus, found_indexes_list, additional_path)\n",
        "  return vero / len(claim_list)"
      ],
      "metadata": {
        "id": "jwmYTSmWwhl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# method to save in a file the new corpus and the matches to confront\n",
        "tfidf_tresholder_path = \"/content/gdrive/MyDrive/Colab Notebooks/fakenews_data/k_check/\"\n",
        "def confronto(k, matches, originals, found, additional_path):\n",
        "  res = []\n",
        "  new_matches = []\n",
        "\n",
        "  for singular_query_matches in matches:\n",
        "    new_singular_query_matches = []\n",
        "    for doc in singular_query_matches:\n",
        "      new_singular_query_matches.append(\" \".join(doc))\n",
        "    new_matches.append(new_singular_query_matches)\n",
        "\n",
        "  for idx in range(len(dev_data)):\n",
        "    obj = {\n",
        "        \"query\" : dev_data[idx][\"statement\"],\n",
        "        \"top\" : new_matches[idx],\n",
        "        \"original\": \" \".join(originals[idx]),\n",
        "        \"found\" : found[idx]\n",
        "    }\n",
        "    res.append(obj)\n",
        "\n",
        "  with open(tfidf_tresholder_path + additional_path +str(k) + \"/top_docs.json\", \"w\") as F:\n",
        "    json.dump(res, F, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "qrHfwSAPO668"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_check(1, tokenized_corpus, \"no_preprocessing/\"))\n",
        "print(k_check(2, tokenized_corpus, \"no_preprocessing/\"))\n",
        "print(k_check(3, tokenized_corpus, \"no_preprocessing/\"))\n",
        "print(k_check(4, tokenized_corpus, \"no_preprocessing/\"))\n",
        "print(k_check(5, tokenized_corpus, \"no_preprocessing/\"))\n",
        "print(k_check(10, tokenized_corpus, \"no_preprocessing/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJvlyhqGresV",
        "outputId": "3216951d-fa38-4a00-e816-202036278eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9293785310734464\n",
            "0.9519774011299436\n",
            "0.9717514124293786\n",
            "0.9745762711864406\n",
            "0.9774011299435028\n",
            "0.9887005649717514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2)**"
      ],
      "metadata": {
        "id": "4Me0mSRRek5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer_obj = SnowballStemmer(\"italian\")"
      ],
      "metadata": {
        "id": "dFWIWZUM4voy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_stemmed_corpus = []\n",
        "for doc in tokenized_corpus:\n",
        "  tokenized_stemmed_corpus.append([snowball_stemmer_obj.stem(token) for token in doc])"
      ],
      "metadata": {
        "id": "3If2U6jZs0Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = BM25Plus(tokenized_stemmed_corpus)"
      ],
      "metadata": {
        "id": "I_LiGGNGtbHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_list = []\n",
        "for art in dev_data:\n",
        "  tokenized_query = wordpunct_tokenize(art[\"statement\"])\n",
        "  stemmed_tok_query = [snowball_stemmer_obj.stem(token) for token in tokenized_query]\n",
        "  claim_list.append(stemmed_tok_query)"
      ],
      "metadata": {
        "id": "d7nB90Gqt9Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_check(1, tokenized_stemmed_corpus, \"stem/\"))\n",
        "print(k_check(2, tokenized_stemmed_corpus, \"stem/\"))\n",
        "print(k_check(3, tokenized_stemmed_corpus, \"stem/\"))\n",
        "print(k_check(4, tokenized_stemmed_corpus, \"stem/\"))\n",
        "print(k_check(5, tokenized_stemmed_corpus, \"stem/\"))\n",
        "print(k_check(10, tokenized_stemmed_corpus, \"stem/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK8vfZ_Kv55o",
        "outputId": "66d52be7-6a7e-4da0-f6f7-980b82dc17fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9180790960451978\n",
            "0.940677966101695\n",
            "0.96045197740113\n",
            "0.9717514124293786\n",
            "0.9717514124293786\n",
            "0.9858757062146892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3)"
      ],
      "metadata": {
        "id": "h9qfNfJcwFzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_stemmed_corpus = []\n",
        "for doc in tokenized_corpus:\n",
        "  stop_stemmed_corpus.append([snowball_stemmer_obj.stem(token) for token in doc if token not in it_stopwords])"
      ],
      "metadata": {
        "id": "DDNLoEyCwFOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = BM25Plus(stop_stemmed_corpus)"
      ],
      "metadata": {
        "id": "foPYzzNhwo45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_list = []\n",
        "for art in dev_data:\n",
        "  tokenized_query = wordpunct_tokenize(art[\"statement\"])\n",
        "  stop_stemmed_tok_query = [snowball_stemmer_obj.stem(token) for token in tokenized_query if token not in it_stopwords]\n",
        "  claim_list.append(stop_stemmed_tok_query)"
      ],
      "metadata": {
        "id": "Ib9V6QztwwaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_check(1, stop_stemmed_corpus, \"stop_stem/\"))\n",
        "print(k_check(2, stop_stemmed_corpus, \"stop_stem/\"))\n",
        "print(k_check(3, stop_stemmed_corpus, \"stop_stem/\"))\n",
        "print(k_check(4, stop_stemmed_corpus, \"stop_stem/\"))\n",
        "print(k_check(5, stop_stemmed_corpus, \"stop_stem/\"))\n",
        "print(k_check(10, stop_stemmed_corpus, \"stop_stem/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER4bnvOQxJYj",
        "outputId": "90f2623f-3f4a-43f6-d19b-7c38dee6064b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9265536723163842\n",
            "0.9463276836158192\n",
            "0.963276836158192\n",
            "0.9745762711864406\n",
            "0.9745762711864406\n",
            "0.9830508474576272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4)\n",
        "min_df e max_df\n",
        "togli stopwords alla query"
      ],
      "metadata": {
        "id": "oNfXqabRyC_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(stop_words = it_stopwords)#, max_df = 0.50, min_df = 0.01)"
      ],
      "metadata": {
        "id": "2bAnjUJD9eyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfWeighted_tokenized_corpus = []\n",
        "tfidf_matrix = tfidf.fit_transform(corpus)\n",
        "tfidf_tokenizer = tfidf.build_tokenizer()"
      ],
      "metadata": {
        "id": "y4pMbT7jyF3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treshold = 0.25"
      ],
      "metadata": {
        "id": "l3ObrjBnuvr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stripping(corpus_row, relevant_words_idx):\n",
        "  #using the tokenizer from the tfidf to assure correspondance between the tokens\n",
        "  tokenized_row = tfidf_tokenizer(corpus_row)\n",
        "  relevant_words = np.array(tokens)[relevant_words_idx]\n",
        "  new_tokenized_corpus = [tokens for tokens in tokenized_row if tokens in relevant_words]\n",
        "  return new_tokenized_corpus"
      ],
      "metadata": {
        "id": "DwNXVTeGteh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf.get_feature_names_out())\n",
        "tokens = tfidf.get_feature_names_out()\n",
        "stripped_corpus = []\n",
        "\n",
        "for idx in range(len(corpus)):\n",
        "  scores_row = np.array(df.iloc[idx])\n",
        "  idx_tokens_over_treshold = np.argwhere(scores_row > treshold)\n",
        "  stripped_corpus.append(stripping(corpus[idx], idx_tokens_over_treshold))"
      ],
      "metadata": {
        "id": "cwcoIe4TC4IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = BM25Plus(stripped_corpus)"
      ],
      "metadata": {
        "id": "w0-dCDQX16FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_list = [tfidf_tokenizer(article[\"statement\"]) for article in dev_data]"
      ],
      "metadata": {
        "id": "JNKrTSgD2M3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t = 0"
      ],
      "metadata": {
        "id": "j48DV7T7-Gs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "esempio di corpus:"
      ],
      "metadata": {
        "id": "16jdTFtaBzkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(stripped_corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "vfDEYdNjB3Ei",
        "outputId": "6dfe299c-bca5-4435-e7d8-3432be48ac55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'28 aprile ex ministro commentato completamento struttura nuovo anno otto mesi circa crollo viadotto autostradale avvenuto 14 agosto 2018 ponte fatto pagare doveva gestire assenza manutenzione fatto crollare tempi stati rispettati ex ministro ragione no verificato questione tempistiche ricostruzione ponte febbraio 2019 verificato state mantenute alcune promesse fatte autorità tempistiche demolizione rimasto piedi viadotto quotidiano invece raccolto quasi 100 promesse ricostruzione fatte 14 agosto 2018 22 aprile 2020 membri scorso governo ministro amministrazione regionale commissario straordinario ricostruzione sindaco mostra grafico interattivo sito promesse numero giorni richiesto ricostruzione momento crollo spaziano circa 170 giorni annunciati 15 agosto 2018 ponte pronto febbraio 2019 circa 870 giorni annunciati 13 novembre 2018 ponte pronto dicembre 2020 tutte promesse fatte partire 2019 fino scoppio emergenza coronavirus indicavano data probabile termine lavori marzo aprile 2020 18 gennaio 2019 occasione firma contratto demolizione ricostruzione nuovo ponte detto contratto consegna ultima lavori indicato 15 aprile 2020 generale circa 70 cento promesse raccolte tempistiche ricostruzione nuovo ponte state mantenute esempio 18 dicembre 2018 allora ministro promesso nuovo ponte stato piedi fine 2019 inaugurato inizio 2020 anzi magari addirittura fine dicembre 2019 complesso base media date contenute stime dichiarate passato ex ministro raccolte nuovo ponte dovuto essere pronto 546 giorni crollo quindi circa tre mesi fa discorso analogo mancate promesse vale prendono considerazione solo ultime settimane caratterizzate emergenza coronavirus durante quali lavori ricostruzione comunque proseguiti 23 marzo scrive imprese impegnate ricostruzione ribadito impegno onorare scadenze annunciate nonostante complicazioni provocate emergenza coronavirus entro 15 aprile dovrebbero concludere operazioni quota mantenuto due settimane dopo data indicata aprile invece annunciato struttura nuovo ponte stata pronta 21 aprile cosa avvenuta realtà 28 aprile poi completamento struttura ossia posizionamento piano stradale impalcato corrisponde realizzazione definitiva opera prossime settimane altre cose inizieranno infatti lavori posa asfalto impianti illuminazione vista fase collaudi contattato ufficio stampa commissario straordinario ricostruzione indicato incirca metà luglio possibile periodo inaugurazione nuovo viadotto visto dunque esagerato dire fa tempi stati rispettati ministro aggiunto nuovo ponte pagato doveva gestire assenza manutenzione fatto crollare entrare merito responsabilità società gestisce tratto autostradale questione magistratura indagando processo primo grado ancora iniziato po chiarezza deve pagare ricostruzione ponte cosiddetto decreto 109 28 settembre 2018 convertito legge novembre 2018 stabilito art co dovesse far fronte spese ricostruzione altro stato consentito partecipare lavori stati assegnati caso mancato ritardato versamento parte concessionario decreto autorizzava spesa complessiva 360 milioni euro 30 milioni euro annui 2018 2029 presi risorse investimenti sviluppo infrastrutturale assicurare realizzazione lavori contattato ufficio stampa commissario straordinario ricostruzione confermato fino oggi regolarmente pagato somme dovute avanzamento lavori comunicato stampa pubblicato 28 aprile confermato pagamenti regolari spiegando 2019 stati erogati 280 milioni euro attività demolizione ricostruzione totale oltre 520 milioni euro accantonamenti oneri complessivamente sostenuti biennio 2018 2019 però ancora piedi scontro governo concessionaria fatto ricorso essere stata estromessa attività ricostruzione ponte senza però chiedere sospensione lavori inizio aprile scorso questione vaglio costituzionale dovrà esprimersi eventuali profili illegittimità costituzionali presenti decreto infine scorso governo promesso revocare concessione autostradale impegno mantenuto mentre nuovo esecutivo impegnato programma avviare revisione concessioni autostradali oggi partita ancora aperta scritto passato comunque stima revoca concessioni potrebbe costare italiano 15 20 miliardi euro nuovo tempi stati rispettati fatto pagare doveva gestire assenza manutenzione fatto crollare riguarda tempi esagera analisi decine decine promesse fatte diversi politici data fine lavori peraltro ancora avvenuta mostra tempi stati po lunghi considerando recente emergenza coronavirus vero invece decreto scorso governo escluso concessionaria gestisce tratto strada crollato ricostruzione ponte obbligandola però pagamento ufficio stampa commissario straordinario ricostruzione comunicato confermato fino oggi regolarmente pagando dovuto avanzamento lavori comunque piedi ricorso società decreto mentre sembra essere stata accantonata ipotesi revoca concessioni conclusione merita'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_check(1, stripped_corpus, \"tfidf/0/\"))\n",
        "print(k_check(2, stripped_corpus, \"tfidf/0/\"))\n",
        "print(k_check(3, stripped_corpus, \"tfidf/0/\"))\n",
        "print(k_check(4, stripped_corpus, \"tfidf/0/\"))\n",
        "print(k_check(5, stripped_corpus, \"tfidf/0/\"))\n",
        "print(k_check(10, stripped_corpus, \"tfidf/0/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyuMIw80-H7m",
        "outputId": "54be1357-f491-4809-8a82-ce065ebba44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8757062146892656\n",
            "0.8983050847457628\n",
            "0.9096045197740112\n",
            "0.9180790960451978\n",
            "0.9209039548022598\n",
            "0.943502824858757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run with treshold = 0.05\n"
      ],
      "metadata": {
        "id": "icmXzq2I26eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "esempio di corpus:"
      ],
      "metadata": {
        "id": "Wdba1xDbB6L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(stripped_corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "iVoZrccWB8Eu",
        "outputId": "0b06781b-946f-401d-bf07-a9baeba643cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'28 aprile ministro struttura nuovo crollo viadotto autostradale 2018 ponte fatto pagare doveva gestire assenza manutenzione fatto crollare tempi stati rispettati ministro tempistiche ricostruzione ponte 2019 promesse fatte tempistiche demolizione piedi viadotto promesse ricostruzione fatte 2018 aprile 2020 ministro commissario straordinario ricostruzione promesse ricostruzione crollo 2018 ponte pronto 2019 2018 ponte pronto 2020 promesse fatte 2019 emergenza coronavirus lavori aprile 2020 2019 demolizione ricostruzione nuovo ponte lavori aprile 2020 promesse tempistiche ricostruzione nuovo ponte 2018 ministro nuovo ponte piedi 2019 2020 2019 ministro nuovo ponte pronto crollo promesse emergenza coronavirus lavori ricostruzione ricostruzione impegno emergenza coronavirus aprile aprile struttura nuovo ponte aprile 28 aprile struttura lavori ufficio commissario straordinario ricostruzione nuovo viadotto tempi stati rispettati ministro nuovo ponte doveva gestire assenza manutenzione fatto crollare autostradale pagare ricostruzione ponte decreto 28 2018 2018 ricostruzione lavori stati decreto 2018 lavori ufficio commissario straordinario ricostruzione confermato lavori 28 aprile confermato 2019 stati demolizione ricostruzione 2018 2019 piedi concessionaria fatto ricostruzione ponte lavori aprile decreto autostradale impegno nuovo concessioni concessioni nuovo tempi stati rispettati fatto pagare doveva gestire assenza manutenzione fatto crollare tempi promesse fatte lavori tempi stati emergenza coronavirus decreto concessionaria ricostruzione ponte ufficio commissario straordinario ricostruzione confermato lavori piedi decreto concessioni'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_check(1, stripped_corpus, \"tfidf/005/\"))\n",
        "print(k_check(2, stripped_corpus, \"tfidf/005/\"))\n",
        "print(k_check(3, stripped_corpus, \"tfidf/005/\"))\n",
        "print(k_check(4, stripped_corpus, \"tfidf/005/\"))\n",
        "print(k_check(5, stripped_corpus, \"tfidf/005/\"))\n",
        "print(k_check(10, stripped_corpus, \"tfidf/005/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqS7IHNb2ak5",
        "outputId": "1d6b833f-22a8-448f-cbdb-4acfa7b42294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8022598870056498\n",
            "0.8531073446327684\n",
            "0.8813559322033898\n",
            "0.8870056497175142\n",
            "0.8983050847457628\n",
            "0.943502824858757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run with treshold = 0.1"
      ],
      "metadata": {
        "id": "9MHMno9h27AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "esempio di corpus:"
      ],
      "metadata": {
        "id": "xLmlRGNRCpIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(stripped_corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "409bdcca-6886-411c-9deb-daabc97cc394",
        "id": "79Y0cV7_CpIB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aprile nuovo 2018 ponte ricostruzione ponte promesse promesse ricostruzione 2018 aprile ricostruzione promesse ricostruzione 2018 ponte 2018 ponte promesse lavori aprile ricostruzione nuovo ponte lavori aprile promesse ricostruzione nuovo ponte 2018 nuovo ponte nuovo ponte promesse lavori ricostruzione ricostruzione aprile aprile nuovo ponte aprile aprile lavori ricostruzione nuovo nuovo ponte ricostruzione ponte 2018 2018 ricostruzione lavori 2018 lavori ricostruzione lavori aprile ricostruzione 2018 ricostruzione ponte lavori aprile nuovo nuovo promesse lavori ricostruzione ponte ricostruzione lavori'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_check(1, stripped_corpus, \"tfidf/01/\"))\n",
        "print(k_check(2, stripped_corpus, \"tfidf/01/\"))\n",
        "print(k_check(3, stripped_corpus, \"tfidf/01/\"))\n",
        "print(k_check(4, stripped_corpus, \"tfidf/01/\"))\n",
        "print(k_check(5, stripped_corpus, \"tfidf/01/\"))\n",
        "print(k_check(10, stripped_corpus, \"tfidf/01/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z57Fp5Nd2AE8",
        "outputId": "71c391ff-0de5-4033-bc73-27f9d0aad105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.711864406779661\n",
            "0.7768361581920904\n",
            "0.807909604519774\n",
            "0.8361581920903954\n",
            "0.8389830508474576\n",
            "0.8813559322033898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run with treshold = 0.25"
      ],
      "metadata": {
        "id": "yDbxcUnJ2myf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "esempio di corpus:"
      ],
      "metadata": {
        "id": "NjSWHKD_2myt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(stripped_corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "391b4184-d9e7-46ac-f8ba-91684a1353d9",
        "id": "efaYe90-2myu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ponte ricostruzione ponte ricostruzione ricostruzione ricostruzione ponte ponte ricostruzione ponte ricostruzione ponte ponte ponte ricostruzione ricostruzione ponte ricostruzione ponte ricostruzione ponte ricostruzione ricostruzione ricostruzione ricostruzione ponte ricostruzione ponte ricostruzione'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(k_check(1, stripped_corpus, \"tfidf/025/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddfa256-b23c-4123-ded9-ae27ebd95182",
        "id": "RZYtHL6v2myu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3983050847457627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- fai solo sul test data V\n",
        "\n",
        "- check: fagli stampare almeno una volta query e doc di risposta V\n",
        "\n",
        "- divid i claim in frasi\n",
        "\n",
        "- controlla tutto! stampa su un file i vari check per verificare V"
      ],
      "metadata": {
        "id": "b-ks8ESRl_PV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# divid i claim in frasi"
      ],
      "metadata": {
        "id": "f4GqfAkMS3FY"
      }
    }
  ]
}